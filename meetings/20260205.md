# `oceanmodeling` Bimonthly Meeting Summary – Feb 5, 2026

## Agenda 
1. Community management
2. RTide: Automating the Tidal Response Method by @thomasmonahan (University of Oxford)
3. Using Transformers for flood modeling by Amitay Sicherman and @gauchm (Google Research)
4. [If time allows] `ioc_cleanup`: reproducible cleaning of tide gauges by EC-JRC

## 1. Thomas Monahan – [Rtide](https://github.com/thomasmonahan/RTide), a Physics-Informed Response Model for Tidal Processes

### The idea

Tides, storm surges, tidal river flooding, tidal currents — these might look like separate phenomena, but Monahan argues they're all fundamentally **tidal processes** driven by the same underlying physics (shallow water equations). His Python package **Rtide** offers a unified, response-based framework to predict all of them, and it runs on just 3 lines of code.

### Why not just use numerical models?

Numerical models are powerful — they solve the shallow water equations, they account for non-local processes, and they're physically grounded. But they have two persistent pain points:

- **Bad bathymetry.** Even in well-surveyed places like the UK and the Netherlands, coastal bathymetry is often too poor for models to nail the local dynamics.
- **Gauge dependency.** Many methods need real-time observations. Gauges go offline, coverage is patchy globally, and if your method breaks without live data, that's a problem for operations.

### The trick: time-invariant response functions

The key insight (going back to Munk & Cartwright in the 1960s) is that tidal responses to forcing are generally **time-invariant and weakly nonlinear**. That means you can learn a fixed relationship between inputs (gravitational forcing, wind, pressure, river flow...) and outputs (sea level, currents) from historical data, and then use it to predict forward — **no real-time observations needed**.

The catch with the classical formulation is that nonlinear interactions blow up combinatorially (Volterra series). Thomas' solution: replace the Volterra series with an equivalent **Volterra neural network** (three-layer feed forward neural network) that learns the interaction kernels directly from data via gradient descent. Importantly, this isn't a black-box ML hack — there's a proven mathematical equivalence between the network and the Volterra operator, so the physics is fully retained and you can pull the model apart to inspect the learned response functions.

### Case studies

**Tidal rivers (River Dee, UK):**
Classical harmonic analysis completely falls apart on the non-stationary signal created by tide–river interaction. Rtide handles it well, using only gravitational forcing and upstream river levels. Every prediction is independent (no autoregression), and the model also works with modeled river runoff instead of in-situ observations.

**Storm surge & nuisance flooding (Money Point, Virginia):**
Adding meteorological variables as inputs, the model predicts total sea level during compound events. As a sanity check, it recovers the inverse barometric pressure effect purely from observational data.

**Operational forecasting (UK Environment Agency + Dutch water authorities):**
This is where things get really practical. Used as a **post-processing layer** on top of existing numerical models (AMM7, DCSM7), Rtide cuts forecast errors by **>60%** — with a purely time-invariant correction. The standalone version (no numerical model, just gravity + weather trained on gauge data) can even beat the UK's operational 7km and 1.5km models on average metrics and Brier scores. But numerical models still win for non-locally generated extremes, so the post-processing combo is the sweet spot. And it's computationally cheap.

**Tidal currents & energy (Pentland Firth, Scotland):**
Standard harmonic analysis treats U and V velocity components independently — which is fine in gentle flows but breaks down in the fast, nonlinear currents that matter for tidal energy. Rtide's **coupled architecture** (jointly estimating U and V) gives ~50% reduction in forecasted power error at MeyGen, the world's largest tidal energy site. It also needs much less training data to outperform harmonic analysis. A SIREN variant (sinusoidal activations) pushes performance further for high-frequency signals, though it's a bit more finicky to train.

**Continental US benchmarking:**
Across 40 NOAA gauges, Rtide averages ~9% improvement over harmonic analysis, with the biggest gains in fast, nonstationary currents.

### Package & licensing

Rtide is on GitHub. Currently, Business Source License free for academic/personal use. Licensing is being sorted out with Oxford — the goal is more open licensing terms, more info coming in the future. If in doubt, email Thomas.

More info about the package and the publication directly on GitHub Rtide page: https://github.com/thomasmonahan/RTide

---

## 2. Amitay Sicherman (Google Research) – Global Sea Level Prediction with Deep Learning

### The idea

Can a single deep learning model, trained on tide gauge data from around the world, predict sea level **everywhere** — including places with no gauges? 
Google flood forecasting team (Amitay, Martin, Patrick) shows how Deep Learning models can outperform traditional hydrodynamic models.

### The data challenge

Using the **GESLA dataset**: ~1 billion hourly data points from ~6,000 gauges worldwide. lots of stations, but the geographic distribution is wildly uneven — the US, Japan, and Europe are densely covered, while large parts of Africa, South America, and Southeast Asia have almost nothing. And those data-poor regions are often the ones most vulnerable to coastal flooding. The whole premise of using deep learning here is that the model can transfer knowledge from data-rich areas to data-poor ones.

### Model architecture

**Transformer** with four input streams, each handled by a specialized encoder:

| Input type | What it includes | Encoder |
|---|---|---|
| Dynamic gridded | Wind, pressure — 150km radius, 24h history | Vision Transformer (patchify) |
| Static gridded | Bathymetry, land/sea mask | CNN |
| Tidal signal | Astronomical tides or hydrodynamic model output (e.g., GTSM) | MLP |
| Static station attributes | Per-station features | MLP |

All inputs get encoded into a shared latent space, concatenated into a token sequence, and fed through the Transformer. Output: a single sea level value. Think of it like an NLP model, but instead of words, the "tokens" are spatial patches, bathymetry features, and tidal signals.

Additionally: an **hydrodynamic model output** (like GTSM) is fed as an input, and the DL model learns to correct its systematic errors — essentially getting a warm start and then improving on it.

### Experimental setup

All results are in the **ungauged setting**: test stations are completely held out during training, along with a 50km buffer to prevent data leakage. Training data: 1980–2012. Testing: later period at unseen stations.

### Results

Compared to Environment Canada's hydrodynamic model (their strongest baseline), the DL model shows consistent improvement across RMSE, R², and Pearson correlation — and this is at stations the model has never seen. A global map shows the DL model winning in most regions. Two notable exceptions:

- **Baltic Sea** — both models perform poorly (it's a genuinely hard region)
- **Japan** — both models already do well, so there's little room for improvement

### Open challenges

Amitay was refreshingly honest about the limitations:

1. **Station diversity is limited.** A billion data points from only 6,000 stations means the model has limited exposure to the full variety of coastline shapes and local dynamics.
2. **"Ungauged" is hard to truly simulate.** Even when you hold out stations, some of your input products (bathymetry grids, reanalysis data) may have been calibrated using those same gauges. It's a pervasive issue across the field, not just for this model.
3. **Evaluation metrics don't focus on what matters most.** RMSE and friends are dominated by normal conditions. The extreme events — the actual floods — get washed out in the averages. Defining what counts as an "extreme event" and building metrics that properly capture performance on them is an active area of work. Different loss functions (Huber loss, etc.) haven't made a big difference yet.

### Computational cost

Martin noted that inference is quite cheap — likely much cheaper than running a complex hydrodynamic model. Training is a one-time cost that they couldn't put a dollar figure on (Google trains internally), but the inference-time resource requirements are modest and don't need large GPUs.

---

Both teams converge on a key finding: **using hydrodynamic model output as an input** (rather than trying to replace it) is a winning strategy. And both agree that extreme event prediction and evaluation remain the hard, unsolved frontier.
